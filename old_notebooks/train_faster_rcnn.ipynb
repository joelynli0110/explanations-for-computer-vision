{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17271,
     "status": "ok",
     "timestamp": 1638091238461,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "fx6PIMBOF6il",
    "outputId": "18c47131-276e-4986-e43f-9b6f420f1834"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1638091260200,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "5Y1ggCnIGIJe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1638091261847,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "IwBa-EZBHDhu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from src.faster_rcnn import fasterrcnn_resnet18_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1638091263150,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "n4qsmBPrHEuu"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ3E5j-WG6ex"
   },
   "source": [
    "# Finetune pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2828,
     "status": "ok",
     "timestamp": 1638091332630,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "W68tjoIfGiq0",
    "outputId": "412d3196-cf61-4707-961e-fdc73f73c381"
   },
   "outputs": [],
   "source": [
    "# load an instance segmentation model pre-trained pre-trained on COCO\n",
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, num_classes=num_classes)\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, num_classes=num_classes)\n",
    "model = fasterrcnn_resnet18_fpn(pretrained_backbone=True, num_classes=num_classes)\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=num_classes)\n",
    "\n",
    "# load model checkpoint\n",
    "# path = os.path.join(os.getcwd(), \"./checkpoints/faster_rcnn_resnet1810_epochs.ckpt\")\n",
    "# checkpoint = torch.load(path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# model.to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7_uzRwAHVFv"
   },
   "source": [
    "# PennFudan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2330,
     "status": "ok",
     "timestamp": 1638091380802,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "9atFLdgwF6iz",
    "outputId": "533b6355-a426-4a91-b8d8-8a49f253f538"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/gdrive/MyDrive/xai-lab/detection\")\n",
    "\n",
    "from pennfudan_dataset import PennFudanDataset, get_transform\n",
    "from utils import collate_fn\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('../PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('../PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1638091391520,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "tzgWAa7mF6i-"
   },
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9) #weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                step_size=3,\n",
    "#                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7378,
     "status": "ok",
     "timestamp": 1638091399693,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "LUrOQkOlJ5q-",
    "outputId": "1e8c4985-2288-4b18-f082-e53736d5fa18"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch==1.10.0\n",
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kwn3Q1cJ7O6",
    "outputId": "47540e60-51fe-4f4a-bfbb-3bac4a49a5ae"
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# let's train it for 20 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    # lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1638092197699,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "jPnSOOYE6gzL"
   },
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1638092199017,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "IJRrF2tMG8LZ"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "result = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "image = ImageDraw.Draw(result)  \n",
    "\n",
    "for bbox in prediction[0][\"boxes\"]:\n",
    "  image.rectangle(bbox.cpu().numpy().tolist(), outline =\"red\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1638092201806,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "TRc3lNFIJTSB",
    "outputId": "c83d25bb-bf34-46ff-a569-ac195bb70491"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRLHCwEKJXDN"
   },
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "path = os.path.join(os.getcwd(), \"..\", \"checkpoints\" ,\"faster_rcnn_resnet18\" + str(20) + \"_epochs.ckpt\")\n",
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1637853603297,
     "user": {
      "displayName": "Sherif Nekkah",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08579354790829099111"
     },
     "user_tz": -60
    },
    "id": "GTtEawo4AhYk",
    "outputId": "fcf36e94-e36b-470c-f5b8-fd3a1b8ca196"
   },
   "outputs": [],
   "source": [
    "# Test loading of checkpoint\n",
    "\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgfLaVm2BBeg",
    "tags": []
   },
   "source": [
    "# Coco Dataset\n",
    "Please follow https://medium.com/howtoai/pytorch-torchvision-coco-dataset-b7f5e8cad82.\n",
    "Do the terminal commands in *explanations-for-computer-vision* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class myOwnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.66s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)# path to your own data and coco file\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "path2train=\"./coco/train2017\"\n",
    "path2trainjson=\"./coco/annotations/instances_train2017.json\"\n",
    "path2val=\"./coco/val2017\"\n",
    "path2valjson=\"./coco/annotations/instances_val2017.json\"\n",
    "# create own Dataset\n",
    "my_dataset = myOwnDataset(root=path2train,\n",
    "                          annotation=path2trainjson,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "\n",
    "my_dataset_test = myOwnDataset(root=path2val,\n",
    "                          annotation=path2valjson,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "# Batch size\n",
    "train_batch_size = 1\n",
    "\n",
    "# own DataLoader\n",
    "data_loader = torch.utils.data.DataLoader(my_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(my_dataset_test,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  118287\n",
      "Number of samples:  5000\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(data_loader))\n",
    "print('Number of samples: ', len(data_loader_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sherif/.local/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/118287, Loss: 5.266780853271484\n",
      "Iteration: 2/118287, Loss: 6.004335880279541\n",
      "Iteration: 3/118287, Loss: 5.346428871154785\n",
      "Iteration: 4/118287, Loss: 5.199862480163574\n",
      "Iteration: 5/118287, Loss: 5.3045854568481445\n",
      "Iteration: 6/118287, Loss: 5.1054840087890625\n",
      "Iteration: 7/118287, Loss: 5.000123977661133\n",
      "Iteration: 8/118287, Loss: 4.975391387939453\n",
      "Iteration: 9/118287, Loss: 4.818087577819824\n",
      "Iteration: 10/118287, Loss: 4.624713897705078\n",
      "Iteration: 11/118287, Loss: 4.820614337921143\n",
      "Iteration: 12/118287, Loss: 4.6482014656066895\n",
      "Iteration: 13/118287, Loss: 4.1555705070495605\n",
      "Iteration: 14/118287, Loss: 4.493748664855957\n",
      "Iteration: 15/118287, Loss: 3.89536452293396\n",
      "Iteration: 16/118287, Loss: 4.109428405761719\n",
      "Iteration: 17/118287, Loss: 3.344741106033325\n",
      "Iteration: 18/118287, Loss: 3.095829486846924\n",
      "Iteration: 19/118287, Loss: 3.3950626850128174\n",
      "Iteration: 20/118287, Loss: 2.9761412143707275\n",
      "Iteration: 21/118287, Loss: 2.200562000274658\n",
      "Iteration: 22/118287, Loss: 2.3220341205596924\n",
      "Iteration: 23/118287, Loss: 1.9253060817718506\n",
      "Iteration: 24/118287, Loss: 2.3271119594573975\n",
      "Iteration: 25/118287, Loss: 1.870239496231079\n",
      "Iteration: 26/118287, Loss: 1.6499179601669312\n",
      "Iteration: 27/118287, Loss: 1.8661649227142334\n",
      "Iteration: 28/118287, Loss: 3.4043476581573486\n",
      "Iteration: 29/118287, Loss: 1.545058012008667\n",
      "Iteration: 30/118287, Loss: 2.927908182144165\n",
      "Iteration: 31/118287, Loss: 1.1948655843734741\n",
      "Iteration: 32/118287, Loss: 2.0264384746551514\n",
      "Iteration: 33/118287, Loss: 1.933891773223877\n",
      "Iteration: 34/118287, Loss: 2.0128941535949707\n",
      "Iteration: 35/118287, Loss: 1.1834702491760254\n",
      "Iteration: 36/118287, Loss: 1.551484227180481\n",
      "Iteration: 37/118287, Loss: 1.6146371364593506\n",
      "Iteration: 38/118287, Loss: 1.9445805549621582\n",
      "Iteration: 39/118287, Loss: 2.3618617057800293\n",
      "Iteration: 40/118287, Loss: 3.7548630237579346\n",
      "Iteration: 41/118287, Loss: 1.5498101711273193\n",
      "Iteration: 42/118287, Loss: 1.5562934875488281\n",
      "Iteration: 43/118287, Loss: 2.008305311203003\n",
      "Iteration: 44/118287, Loss: 1.7127368450164795\n",
      "Iteration: 45/118287, Loss: 2.0980021953582764\n",
      "Iteration: 46/118287, Loss: 1.5253779888153076\n",
      "Iteration: 47/118287, Loss: 2.2722818851470947\n",
      "Iteration: 48/118287, Loss: 2.2652549743652344\n",
      "Iteration: 49/118287, Loss: 1.1654199361801147\n",
      "Iteration: 50/118287, Loss: 2.116424560546875\n",
      "Iteration: 51/118287, Loss: 0.9748380184173584\n",
      "Iteration: 52/118287, Loss: 1.8388283252716064\n",
      "Iteration: 53/118287, Loss: 1.6900306940078735\n",
      "Iteration: 54/118287, Loss: 0.8917699456214905\n",
      "Iteration: 55/118287, Loss: 0.7783293128013611\n",
      "Iteration: 56/118287, Loss: 1.4590280055999756\n",
      "Iteration: 57/118287, Loss: 0.6201836466789246\n",
      "Iteration: 58/118287, Loss: 1.0180995464324951\n",
      "Iteration: 59/118287, Loss: 1.2640156745910645\n",
      "Iteration: 60/118287, Loss: 0.8507541418075562\n"
     ]
    }
   ],
   "source": [
    "# 2 classes; Only target class or background\n",
    "num_classes = 91\n",
    "num_epochs = 1\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        if annotations != []:\n",
    "            loss_dict = model(imgs, annotations)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = my_dataset[99]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "result = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "image = ImageDraw.Draw(result)  \n",
    "\n",
    "for bbox in prediction[0][\"boxes\"]:\n",
    "    image.rectangle(bbox.cpu().numpy().tolist(), outline =\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "path = os.path.join(os.getcwd(), \"..\", \"checkpoints\" ,\"faster_rcnn_resnet50_\" + str(1) + \"_epoch_pretrained_COCO.ckpt\")\n",
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_faster_rcnn.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
